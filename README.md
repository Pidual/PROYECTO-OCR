
---

# ğŸ§  OCR para CarnÃ©s Estudiantiles

Este proyecto implementa un sistema de Reconocimiento Ã“ptico de Caracteres (OCR) para extraer informaciÃ³n estructurada de carnÃ©s estudiantiles, utilizando modelos de visiÃ³n integrados con [Ollama](https://ollama.com/).

> âš ï¸ *Honestamente, el cÃ³digo es una colecciÃ³n de scripts pegados con cinta... pero funciona. Milagrosamente.*

---

## ğŸ“‹ DescripciÃ³n

El sistema procesa imÃ¡genes de carnÃ©s estudiantiles y extrae informaciÃ³n como:

* ğŸ§‘â€ğŸ“ Nombre del estudiante
* ğŸ†” CÃ³digo o ID del estudiante
* ğŸ§­ Carrera o programa
* ğŸ›ï¸ InstituciÃ³n educativa

EstÃ¡ compuesto por:

* Una API
* Una cola de mensajes (RabbitMQ)
* Un worker que utiliza el modelo `qwen2.5vl:7b` para procesar las imÃ¡genes

> ğŸ¯ *De los 9 modelos probados, `qwen2.5vl:7b` fue el Ãºnico que dijo â€œsÃ­â€ a CPU + GPU. Toda una joya.*

---

## ğŸ–¥ï¸ Requisitos del Sistema

* Python `3.8+`
* NVIDIA GPU con al menos `6GB VRAM` (âš ï¸ Si tienes menos, el script explotarÃ¡)
* Docker (opcional, recomendado para RabbitMQ)
* Ollama `0.7.0+`

> ğŸ’¡ Si necesitas usar un modelo mÃ¡s liviano, puedes hacer downgrade a `llava:7b`, pero los resultados son bastante pobres.

---

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio:**

```bash
git clone https://github.com/Pidual/PROYECTO-OCR.git
cd PROYECTO-OCR
```

2. **Crear entorno virtual e instalar dependencias:**

```bash
python -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

3. **Instalar Ollama (Linux):**

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

4. **Instalar RabbitMQ (usando Docker):**

```bash
docker run -d --name rabbitmq -p 5673:5672 -p 15673:15672 rabbitmq:management
```

5. **Descargar modelo de visiÃ³n recomendado:**

```bash
ollama pull qwen2.5vl:7b
```

---

## âš™ï¸ ConfiguraciÃ³n

Crea un archivo `.env` en la raÃ­z del proyecto con el siguiente contenido:

```env
# ConfiguraciÃ³n bÃ¡sica
DEBUG=True
PORT=5000
HOST=0.0.0.0

# RabbitMQ
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5673
RABBITMQ_QUEUE=ocr_queue

# Ollama
OLLAMA_API_URL=http://localhost:11434/api
```

> ğŸ”’ La variable `OLLAMA_MODEL` estÃ¡ *hardcoded* en la lÃ­nea 11 de `ocr_processor.py`.

---

## ğŸš€ Uso

Iniciar el sistema (usando modelo por defecto):

```bash
./run.py --all --monitor
```

> TambiÃ©n puedes especificar el modelo directamente.

---

## ğŸ–¼ï¸ Enviar imÃ¡genes para procesamiento

Ejemplo de uso visual:

![image](https://github.com/user-attachments/assets/ce528db0-6365-41f3-87e5-3deb6432f974)

Resultado esperado:
![image](https://github.com/user-attachments/assets/031e2d7f-a391-4087-9ebd-ed91dcc277a9)

---

## ğŸ” SoluciÃ³n de problemas

**Errores de GPU:**

* Si tu GPU no tiene suficiente VRAM, cambia a un modelo mÃ¡s liviano (aunque los resultados serÃ¡n meh ğŸ˜).
* No olvides actualizar el modelo en el `.env`.

**Resultados incorrectos:**

* Prueba con otros modelos disponibles.
* Ajusta el flujo en `ocr_processor.py`.

---

## ğŸ“Š Ejemplos

IntegraciÃ³n con una aplicaciÃ³n web:

> *(AquÃ­ podrÃ­as agregar mÃ¡s ejemplos o capturas si deseas.)*

---

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la licencia MIT.

---

Si quieres, tambiÃ©n puedo ayudarte a hacer una tabla de modelos probados con sus pros y contras. Â¿Te gustarÃ­a incluir eso en el README?
